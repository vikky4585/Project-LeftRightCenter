{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('data/liwc_raw_scores_full.csv') \n",
    "df = pd.read_csv('data/liwc_categorical_scores_full.csv') \n",
    "\n",
    "X = df.drop(['party'], axis=1)\n",
    "y = df['party']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>avg_x</th>\n",
       "      <th>avg_y</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cogproc</td>\n",
       "      <td>0.069337</td>\n",
       "      <td>0.055145</td>\n",
       "      <td>0.014193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>function</td>\n",
       "      <td>0.403427</td>\n",
       "      <td>0.389612</td>\n",
       "      <td>0.013815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relativ</td>\n",
       "      <td>0.131866</td>\n",
       "      <td>0.141712</td>\n",
       "      <td>0.009846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>verb</td>\n",
       "      <td>0.112640</td>\n",
       "      <td>0.102873</td>\n",
       "      <td>0.009767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>social</td>\n",
       "      <td>0.094723</td>\n",
       "      <td>0.086987</td>\n",
       "      <td>0.007735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>work</td>\n",
       "      <td>0.055211</td>\n",
       "      <td>0.062878</td>\n",
       "      <td>0.007667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>auxverb</td>\n",
       "      <td>0.056543</td>\n",
       "      <td>0.049008</td>\n",
       "      <td>0.007535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>posemo</td>\n",
       "      <td>0.039836</td>\n",
       "      <td>0.047024</td>\n",
       "      <td>0.007188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>space</td>\n",
       "      <td>0.069213</td>\n",
       "      <td>0.076143</td>\n",
       "      <td>0.006929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>negemo</td>\n",
       "      <td>0.019961</td>\n",
       "      <td>0.013069</td>\n",
       "      <td>0.006891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pronoun</td>\n",
       "      <td>0.087219</td>\n",
       "      <td>0.080394</td>\n",
       "      <td>0.006826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>focuspresent</td>\n",
       "      <td>0.082545</td>\n",
       "      <td>0.076182</td>\n",
       "      <td>0.006363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>differ</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>0.009130</td>\n",
       "      <td>0.005397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>prep</td>\n",
       "      <td>0.145990</td>\n",
       "      <td>0.150559</td>\n",
       "      <td>0.004568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>discrep</td>\n",
       "      <td>0.012321</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>0.004382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>drives</td>\n",
       "      <td>0.125211</td>\n",
       "      <td>0.120882</td>\n",
       "      <td>0.004329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ipron</td>\n",
       "      <td>0.032645</td>\n",
       "      <td>0.028706</td>\n",
       "      <td>0.003939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>negate</td>\n",
       "      <td>0.008576</td>\n",
       "      <td>0.004811</td>\n",
       "      <td>0.003766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>conj</td>\n",
       "      <td>0.037378</td>\n",
       "      <td>0.034115</td>\n",
       "      <td>0.003264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>certain</td>\n",
       "      <td>0.013704</td>\n",
       "      <td>0.010450</td>\n",
       "      <td>0.003254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>anger</td>\n",
       "      <td>0.007412</td>\n",
       "      <td>0.004377</td>\n",
       "      <td>0.003035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ppron</td>\n",
       "      <td>0.054571</td>\n",
       "      <td>0.051683</td>\n",
       "      <td>0.002888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>money</td>\n",
       "      <td>0.017411</td>\n",
       "      <td>0.020240</td>\n",
       "      <td>0.002829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>article</td>\n",
       "      <td>0.062792</td>\n",
       "      <td>0.065206</td>\n",
       "      <td>0.002414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>risk</td>\n",
       "      <td>0.011822</td>\n",
       "      <td>0.009420</td>\n",
       "      <td>0.002402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>adverb</td>\n",
       "      <td>0.023291</td>\n",
       "      <td>0.020998</td>\n",
       "      <td>0.002293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>affiliation</td>\n",
       "      <td>0.039629</td>\n",
       "      <td>0.037360</td>\n",
       "      <td>0.002268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>time</td>\n",
       "      <td>0.048218</td>\n",
       "      <td>0.050462</td>\n",
       "      <td>0.002244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>bio</td>\n",
       "      <td>0.014517</td>\n",
       "      <td>0.012424</td>\n",
       "      <td>0.002093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>reward</td>\n",
       "      <td>0.012284</td>\n",
       "      <td>0.014365</td>\n",
       "      <td>0.002081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>achieve</td>\n",
       "      <td>0.024952</td>\n",
       "      <td>0.025556</td>\n",
       "      <td>0.000604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>you</td>\n",
       "      <td>0.008703</td>\n",
       "      <td>0.008118</td>\n",
       "      <td>0.000584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>see</td>\n",
       "      <td>0.006462</td>\n",
       "      <td>0.006953</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>focusfuture</td>\n",
       "      <td>0.011050</td>\n",
       "      <td>0.011535</td>\n",
       "      <td>0.000485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>focuspast</td>\n",
       "      <td>0.021081</td>\n",
       "      <td>0.021565</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>body</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>0.000472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>feel</td>\n",
       "      <td>0.002282</td>\n",
       "      <td>0.001845</td>\n",
       "      <td>0.000436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>informal</td>\n",
       "      <td>0.002467</td>\n",
       "      <td>0.002820</td>\n",
       "      <td>0.000353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>death</td>\n",
       "      <td>0.002318</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>0.000279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>netspeak</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.000277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>sexual</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.000251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>compare</td>\n",
       "      <td>0.017236</td>\n",
       "      <td>0.017451</td>\n",
       "      <td>0.000215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>affect</td>\n",
       "      <td>0.060227</td>\n",
       "      <td>0.060402</td>\n",
       "      <td>0.000175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>nonflu</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.000123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>swear</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>home</td>\n",
       "      <td>0.007076</td>\n",
       "      <td>0.007020</td>\n",
       "      <td>0.000056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>assent</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>0.000044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>ingest</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>friend</td>\n",
       "      <td>0.002623</td>\n",
       "      <td>0.002624</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>filler</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>QMark</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>SemiC</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Quote</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>OtherP</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Apostro</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Exclam</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Dash</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Comma</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Colon</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Parenth</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        features     avg_x     avg_y      diff\n",
       "10       cogproc  0.069337  0.055145  0.014193\n",
       "0       function  0.403427  0.389612  0.013815\n",
       "2        relativ  0.131866  0.141712  0.009846\n",
       "4           verb  0.112640  0.102873  0.009767\n",
       "7         social  0.094723  0.086987  0.007735\n",
       "15          work  0.055211  0.062878  0.007667\n",
       "14       auxverb  0.056543  0.049008  0.007535\n",
       "19        posemo  0.039836  0.047024  0.007188\n",
       "11         space  0.069213  0.076143  0.006929\n",
       "27        negemo  0.019961  0.013069  0.006891\n",
       "8        pronoun  0.087219  0.080394  0.006826\n",
       "9   focuspresent  0.082545  0.076182  0.006363\n",
       "35        differ  0.014526  0.009130  0.005397\n",
       "1           prep  0.145990  0.150559  0.004568\n",
       "41       discrep  0.012321  0.007939  0.004382\n",
       "3         drives  0.125211  0.120882  0.004329\n",
       "23         ipron  0.032645  0.028706  0.003939\n",
       "50        negate  0.008576  0.004811  0.003766\n",
       "22          conj  0.037378  0.034115  0.003264\n",
       "38       certain  0.013704  0.010450  0.003254\n",
       "52         anger  0.007412  0.004377  0.003035\n",
       "16         ppron  0.054571  0.051683  0.002888\n",
       "29         money  0.017411  0.020240  0.002829\n",
       "12       article  0.062792  0.065206  0.002414\n",
       "43          risk  0.011822  0.009420  0.002402\n",
       "25        adverb  0.023291  0.020998  0.002293\n",
       "20   affiliation  0.039629  0.037360  0.002268\n",
       "18          time  0.048218  0.050462  0.002244\n",
       "36           bio  0.014517  0.012424  0.002093\n",
       "42        reward  0.012284  0.014365  0.002081\n",
       "..           ...       ...       ...       ...\n",
       "24       achieve  0.024952  0.025556  0.000604\n",
       "49           you  0.008703  0.008118  0.000584\n",
       "54           see  0.006462  0.006953  0.000491\n",
       "44   focusfuture  0.011050  0.011535  0.000485\n",
       "26     focuspast  0.021081  0.021565  0.000484\n",
       "68          body  0.001897  0.001425  0.000472\n",
       "65          feel  0.002282  0.001845  0.000436\n",
       "63      informal  0.002467  0.002820  0.000353\n",
       "64         death  0.002318  0.002039  0.000279\n",
       "69      netspeak  0.001323  0.001600  0.000277\n",
       "70        sexual  0.000879  0.000628  0.000251\n",
       "31       compare  0.017236  0.017451  0.000215\n",
       "13        affect  0.060227  0.060402  0.000175\n",
       "72        nonflu  0.000471  0.000595  0.000123\n",
       "73         swear  0.000147  0.000078  0.000069\n",
       "53          home  0.007076  0.007020  0.000056\n",
       "71        assent  0.000507  0.000551  0.000044\n",
       "67        ingest  0.002199  0.002171  0.000028\n",
       "62        friend  0.002623  0.002624  0.000002\n",
       "74        filler  0.000014  0.000012  0.000001\n",
       "75         QMark  0.000000  0.000000  0.000000\n",
       "76         SemiC  0.000000  0.000000  0.000000\n",
       "77         Quote  0.000000  0.000000  0.000000\n",
       "78        OtherP  0.000000  0.000000  0.000000\n",
       "79       Apostro  0.000000  0.000000  0.000000\n",
       "80        Exclam  0.000000  0.000000  0.000000\n",
       "81          Dash  0.000000  0.000000  0.000000\n",
       "82         Comma  0.000000  0.000000  0.000000\n",
       "83         Colon  0.000000  0.000000  0.000000\n",
       "84       Parenth  0.000000  0.000000  0.000000\n",
       "\n",
       "[85 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dem = df.loc[df['party'] == 'democrats']\n",
    "df_rep = df.loc[df['party'] == 'republicans']\n",
    "\n",
    "\n",
    "dem_mean = pd.DataFrame({'features': df_dem.mean().index, 'avg': df_dem.mean().values})\n",
    "rep_mean = pd.DataFrame({'features': df_rep.mean().index, 'avg': df_rep.mean().values})\n",
    "\n",
    "dem_mean['avg'] = dem_mean['avg']\n",
    "rep_mean['avg'] = rep_mean['avg']\n",
    "\n",
    "dem_mean.sort_values(by = ['avg'], ascending=False, inplace=True)\n",
    "rep_mean.sort_values(by = ['avg'], ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "df_merged = dem_mean.merge(rep_mean, on='features')\n",
    "\n",
    "df_merged['diff'] = df_merged['avg_x'] - df_merged['avg_y']\n",
    "df_merged['diff'] = df_merged['diff'].abs()\n",
    "df_merged.sort_values(by = ['diff'], ascending=False, inplace=True)\n",
    "df_merged\n",
    "#df_combo = pd.DataFrame(dem_mean['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_feature = np.argsort(dem_mean['avg'])\n",
    "pos = np.arange(sorted_feature.shape[0]) + 0.5\n",
    "plt.barh(pos, dem_mean['avg'], align='center', color='darkgreen')\n",
    "#plt.barh(dem_mean['avg'], dem_mean['features'], align='center', color='darkgreen')\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('Relative Feature Importance')\n",
    "plt.yticks(pos, X.columns[sorted_feature])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=1, stratify=y)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "\n",
    "y_train_encoded = label_encoder.transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "y_train_category = to_categorical(y_train_encoded)\n",
    "y_test_category = to_categorical(y_test_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=100,activation='relu', input_dim=X.shape[1]))\n",
    "model.add(Dense(units=100,activation='relu'))\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "          \n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "result = model.fit(X_train, y_train_category,epochs=100, shuffle=True,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test, y_test_category, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.history['acc'])\n",
    "print(result.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "exDict = {'model_loss': model_loss, 'model_accuracy' : model_accuracy, \n",
    "          'acc':result.history['acc'], 'loss' :result.history['loss']}\n",
    "\n",
    "with open('data/matrix/file.txt', 'w') as file:\n",
    "     file.write(json.dumps(exDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = json.load(open(\"data/matrix/file.txt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"models/raw_full.h5\")\n",
    "#test = np.expand_dims(X_train[0], axis=0)\n",
    "\n",
    "y_prob = model.predict(X_train[:1])\n",
    "model.predict_classes(X_train[:1])\n",
    "print(f\"Predicted class: {model.predict_classes(X_train[5:6])}\")\n",
    "y_classes = y_prob.argmax(axis=-1)\n",
    "y_prob\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"models/raw_full.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.iloc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(X_test[:10])\n",
    "prediction_labels = label_encoder.inverse_transform(predictions)\n",
    "prediction_labels\n",
    "\n",
    "print(f\"Predicted classes: {prediction_labels}\")\n",
    "print(f\"Actual Labels: {list(y_test[:10])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.fit(X_train, y_train_category,epochs=100, shuffle=True,verbose=2)\n",
    "#history = model.fit(X, Y, validation_split=0.33, nb_epoch=150, batch_size=10, verbose=0)\n",
    "print(result.history['acc'])\n",
    "print(result.history['loss'])\n",
    "\n",
    "#print(result.history.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X,y)\n",
    "\n",
    "imp_feature = rf.feature_importances_\n",
    "sorted_feature = np.argsort(imp_feature)\n",
    "print(imp_feature[sorted_feature])\n",
    "pos = np.arange(sorted_feature.shape[0]) + 0.5\n",
    "print(pos)\n",
    "plt.barh(pos, imp_feature[sorted_feature], align='center', color='darkgreen')\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('Relative Feature Importance')\n",
    "plt.yticks(pos, X.columns[sorted_feature])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = model.fit(x, y, validation_split=0.25, epochs=50, batch_size=16, verbose=1)\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
